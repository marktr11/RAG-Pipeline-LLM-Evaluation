{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b7cd94-e2b0-4b97-80dc-6a8d57798fd3",
   "metadata": {},
   "source": [
    "# RAG - Exploration\n",
    "\n",
    "#### This notebook follows the official [LangChain RAG tutorial](https://python.langchain.com/docs/tutorials/rag/) and includes additional exploration, documentation, insights, and observations.  \n",
    "The goal is to gain a solid understanding of how to build a basic RAG pipeline.\n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "1. [Setting up the environment](#1.Setting-up-the-environment)\n",
    "2. [Preview](#2.Preview)\n",
    "3. [Detailed Break down](#3.Detailed-break-down)\n",
    "   \n",
    "    A. [Indexing](#A.Indexing)\n",
    "   \n",
    "    B. [Retrieval and Generation](#B.Retrieval-and-Generation)\n",
    "   \n",
    "5. [Query Analysis](#4.Query-Analysis)\n",
    "\n",
    "\n",
    "\n",
    "## 1.Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a1cc0b4-e8a3-4382-9985-d396cb57a0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aae99c8-8ac2-4af4-a73e-c22361b11938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 15:01:59,207 - INFO - Notebook logging configured.\n",
      "2025-05-06 15:01:59,208 - INFO - Added 'C:\\Users\\minhm\\OneDrive - Université Côte d'Azur\\SERVIER\\src' to sys.path\n",
      "2025-05-06 15:01:59,210 - INFO - Attempting to import 'config' from src...\n",
      "\n",
      "2025-05-06 15:01:59,222 - INFO - Current working directory: C:\\Users\\minhm\\OneDrive - Université Côte d'Azur\\SERVIER\\exploration\n",
      "2025-05-06 15:01:59,224 - INFO - LLM API Key loaded from environment.\n",
      "2025-05-06 15:01:59,225 - INFO - LangSmith API Key loaded.\n",
      "2025-05-06 15:01:59,227 - INFO - LangSmith project name set to: rag_pipeline_llm_evaluation\n",
      "2025-05-06 15:01:59,229 - INFO - PDF file path configured: C:\\Users\\minhm\\OneDrive - Université Côte d'Azur\\SERVIER\\data\\2411.15594v5.pdf\n",
      "2025-05-06 15:01:59,230 - INFO - Successfully imported 'config'.\n"
     ]
    }
   ],
   "source": [
    "# --- Configure logging FIRST ---\n",
    "# So you can see the log messages from config.py when it's imported\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"Notebook logging configured.\") # Optional: Confirm logging setup\n",
    "\n",
    "# --- Add src directory to Python's path ---\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the directory containing the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Go up one level to the project root\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "# Construct the path to the 'src' directory\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "\n",
    "# Check if the 'src' path is already in sys.path to avoid duplicates\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    logging.info(f\"Added '{src_path}' to sys.path\")\n",
    "else:\n",
    "    logging.info(f\"'{src_path}' already in sys.path\")\n",
    "\n",
    "# --- Import config from src ---\n",
    "try:\n",
    "    logging.info(\"Attempting to import 'config' from src...\\n\")\n",
    "    import config # it will display logging from config\n",
    "    logging.info(\"Successfully imported 'config'.\")\n",
    "except ImportError as e:\n",
    "    logging.error(f\"Import error: {e}\")\n",
    "    logging.error(f\"Current sys.path: {sys.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7734d257-38cc-44e2-8790-b07692bbd387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dotenv_path = os.path.join(os.path.dirname(os.getcwd()), 'src\\.env')\n",
    "# print(dotenv_path)\n",
    "# load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d460dba2-15a8-42d1-a976-d875722f7ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U langchain langchain-openai\n",
    "\n",
    "# Set the environment variable for OpenAI API key.\n",
    "# LangChain does not automatically use config.LLM_API_KEY; \n",
    "# it requires the standard environment variable OPENAI_API_KEY to be set.\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = config.LLM_API_KEY\n",
    "\n",
    "from langchain.chat_models import init_chat_model  # Initialize chat model\n",
    "\n",
    "# Use temperature=0 to ensure deterministic output.\n",
    "# By default, temperature > 0 introduces randomness into generation.\n",
    "#\n",
    "# you can select a different chat model as in the tutorial, \n",
    "# but I opted for gpt-3.5-turbo to keep costs low.\n",
    "llm = init_chat_model(\"gpt-3.5-turbo\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61a1f238-e57c-4e3b-9251-cc1ecefcb258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU langchain-openai \n",
    "from langchain_openai import OpenAIEmbeddings # Initialize embedding model\n",
    "\n",
    "# You can choose a different embedding model, as shown in the tutorial, \n",
    "# but I chose a smaller one due to budget constraints.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c09d93-bb2a-4b4b-81cd-5af6cd610dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU langchain-core\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# InMemoryVectorStore is a simple option suitable for exploring RAG concepts,\n",
    "# but for production use, it is recommended to use a more robust vector store.\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffea7b35-db26-4665-9589-708418c36544",
   "metadata": {},
   "source": [
    "## 2.Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4932c490-8041-4086-a793-e5eaab711497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 15:02:00,291 - WARNING - USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "2025-05-06 15:02:05,616 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "if not os.environ.get(\"USER_AGENT\"):\n",
    "    os.environ[\"USER_AGENT\"] = \"my-python-client/5.0\" \n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b0cc7ff-ebcc-40d7-9853-c58f693cd8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 15:02:10,189 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-06 15:02:11,396 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a problem into multiple thought steps, generating multiple thoughts per step in a tree structure. It can be achieved through LLM with simple prompting, task-specific instructions, or human inputs. This approach transforms complex tasks into smaller and simpler steps for easier interpretation and execution by models or agents.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cb52c9-818b-4a02-80f1-13fc63a13d26",
   "metadata": {},
   "source": [
    "These are four answers generated and tutorial answer using `gpt-4o-mini`, `text-embedding-3-large`, and `InMemoryVectorStore`. The first three answers were generated with temperature = 0, while the fourth used the default setting (temperature > 0), which introduces some randomness in the output.\n",
    "\n",
    "- **Answer 1**: Task Decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be achieved through techniques like Chain of Thought (CoT) prompting, which encourages models to think step by step, or by using task-specific instructions. It allows for better planning and execution by clarifying the logical relationships between tasks.\n",
    "\n",
    "- **Answer 2**: Task Decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be achieved through techniques like Chain of Thought (CoT) prompting, which encourages models to think step by step, or by using task-specific instructions. It allows for better planning and execution by clarifying the logical relationships between tasks.\n",
    "\n",
    "- **Answer 3**: Task Decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be achieved through techniques like Chain of Thought (CoT) prompting, which encourages models to think step by step, or by using task-specific instructions. It allows for better planning and execution by clarifying the logical relationships between tasks.\n",
    "\n",
    "- **Answer 4**: Task Decomposition is the process of breaking down complex tasks into smaller, more manageable steps. It can be achieved through various methods, including prompting models to \"think step by step\" or using specific instructions tailored to tasks. This approach enhances performance by enabling more structured reasoning and easier execution of the overall task.\n",
    "\n",
    "- **Tutorial Answer**: Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps to facilitate easier execution and understanding. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) guide models to think step-by-step, allowing them to explore multiple reasoning possibilities. This method enhances performance on complex tasks and provides insight into the model's thinking process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b4b41-5595-4e23-abc4-b6c4aef2b05b",
   "metadata": {},
   "source": [
    "### Observation: Output Variability with Default LLM Settings\n",
    "\n",
    "**Setup & Finding:**\n",
    "*   Executed a RAG pipeline 3 times with identical components:\n",
    "    *   **Chat Model:** `gpt-4o-mini` (via `init_chat_model`)\n",
    "    *   **Embedding:** `text-embedding-3-large`\n",
    "    *   **Vector Store:** `InMemoryVectorStore` (same indexed data)\n",
    "*   **Result:** Each run produced a different answer to the same input question.\n",
    "\n",
    "**Analysis & Root Cause:**\n",
    "*   **Deterministic Components:** Embedding generation and vector search are inherently deterministic.\n",
    "*   **LLM Variability:** The source of variation is the chat model (`gpt-4o-mini`).\n",
    "    *   **Default `temperature`:** When `temperature` is unspecified during initialization (`init_chat_model`), it typically defaults to a value > 0.\n",
    "    *   **Effect:** `temperature > 0` introduces stochasticity (randomness) into the LLM's token selection process during text generation, resulting in varied outputs for identical inputs. ( **Observed behavior differs slightly from the tutorial results due to this randomness.**)\n",
    "\n",
    "**Verification & Solution:**\n",
    "*   **Experiment:** Re-executed the pipeline after explicitly initializing the chat model with `temperature=0`:\n",
    "    ```python\n",
    "    llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0)\n",
    "    ```\n",
    "*   **Outcome:** Setting `temperature=0` resulted in consistent, identical answers across subsequent runs for the same question.\n",
    "\n",
    "**Key Takeaway:**\n",
    "For reproducible LLM outputs, explicitly set `temperature=0`. Be aware that this removes the inherent creativity/diversity associated with higher temperature settings. The choice depends on the desired application behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21be870e-0031-4d3e-8468-d1a26fc04e80",
   "metadata": {},
   "source": [
    "Now, due to resource constraints, I will use `gpt-3.5-turbo` as the chat model and `text-embedding-3-small` for embeddings, while keeping the same vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e08081-9dd5-428f-8cc8-cfc97bef5f37",
   "metadata": {},
   "source": [
    "## 3.Detailed break down\n",
    "### A.Indexing\n",
    "#### A.1 Loading documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "359b1f34-4bf3-4620-8d94-60887e0d1597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters extracted from relevant sections: 43130\n"
     ]
    }
   ],
   "source": [
    "# Goal: Extract only the core content of the blog post, ignoring navigation, footers, etc.\n",
    "# We use BeautifulSoup's SoupStrainer to specify which HTML elements to keep.\n",
    "# Here, we target elements with CSS classes commonly used for blog titles, headers, and main content areas. This is \"what we want to read\" part\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "# Initialize the WebBaseLoader with:\n",
    "# 1. web_paths: A tuple containing the URL(s) to load. Here, it's a specific blog post.\n",
    "# 2. bs_kwargs: A dictionary of keyword arguments to pass directly to the underlying BeautifulSoup parser.\n",
    "#    - \"parse_only\": This argument takes the SoupStrainer defined above. It tells BeautifulSoup\n",
    "#                    to parse *only* the parts of the HTML document that match the strainer criteria.\n",
    "#                    This significantly cleans the input data early in the process.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), # Target URL\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer}, # Apply the content strainer\n",
    ")\n",
    "\n",
    "# The .load() method executes the following steps:\n",
    "# 1. Fetches the HTML content from the specified URL.\n",
    "# 2. Uses BeautifulSoup (configured with the strainer) to parse the HTML.\n",
    "# 3. Extracts the text content from the strained HTML elements.\n",
    "# 4. Packages the extracted text and metadata (like the source URL) into a Langchain Document object.\n",
    "# Returns a list of Document objects (in this case, expected to be just one).\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "# Basic sanity check: Assert that exactly one document was created from the single URL.\n",
    "assert len(docs) == 1, f\"Expected 1 document, but got {len(docs)}\"\n",
    "\n",
    "# Print the length (number of characters) of the extracted page content from the first document.\n",
    "# This provides a quick measure of the amount of relevant text successfully extracted.\n",
    "print(f\"Total characters extracted from relevant sections: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2fa3158-0ce7-4c32-916a-d39947ae9bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the first ~ 980 characters of the first document to inspect the loaded content\n",
    "print(docs[0].page_content[:971])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c77cce3-0f54-46b2-918e-c8d5c2755e37",
   "metadata": {},
   "source": [
    "#### A.2 Splitting documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8dcbf5c-d85a-40ab-b7e6-af077bf1e6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the loaded document(s) into 66 chunks/sub-documents.\n"
     ]
    }
   ],
   "source": [
    "# Import text splitting class from Langchain\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- Text Splitter ---\n",
    "# The goal here is to break down the loaded document(s) (from the previous step)\n",
    "# into smaller, manageable chunks.\n",
    "\n",
    "# Initialize the RecursiveCharacterTextSplitter with specific parameters:\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,       # Defines the target size for each chunk in characters.\n",
    "                           # This value is chosen based on a balance between being small enough\n",
    "                           # for embedding models and large enough to retain meaningful context.\n",
    "                           # It's often tuned based on the embedding model and data characteristics.\n",
    "\n",
    "    chunk_overlap=200,     # Specifies the number of characters that will overlap between\n",
    "                           # consecutive chunks. Overlap helps to preserve context across chunk\n",
    "                           # boundaries, ensuring that sentences or ideas aren't abruptly cut off,\n",
    "                           # which could hinder the understanding of the text.\n",
    "\n",
    "    add_start_index=True,  # If True, this adds metadata to each chunk indicating its\n",
    "                           # starting character index in the original document.\n",
    ")\n",
    "\n",
    "\n",
    "# The `split_documents` method takes the list of Document objects (e.g., 'docs' from the\n",
    "# previous loading step) and applies the configured splitting logic.\n",
    "# `RecursiveCharacterTextSplitter` attempts to split text by a predefined list of\n",
    "# separators (e.g., \"\\n\\n\", \"\\n\", \" \", \"\") in that order, trying to keep semantically\n",
    "# related pieces of text together.\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Print the number of sub-documents (chunks) created after splitting.\n",
    "# This gives an indication of how the original document(s) have been divided.\n",
    "print(f\"Split the loaded document(s) into {len(all_splits)} chunks/sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e122bb-7576-483a-91f2-3a63f3b09e4c",
   "metadata": {},
   "source": [
    "#### A.3 Storing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50165c73-3d37-4f81-85e0-6291a2bb7282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 15:02:15,241 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a73e3ef8-e129-4957-afc8-68f55d7ad48e', '2f6f60a3-1bdf-43af-b6cf-b5caf141c6bd', 'a8b473f2-9449-468d-b691-f5649bef5c78']\n"
     ]
    }
   ],
   "source": [
    "# The `add_documents` method of the vector store object handles this process.\n",
    "# - 'documents=all_splits': This passes the list of chunked Document objects.\n",
    "# - The method iterates through these documents, generates their embeddings (if not already\n",
    "#   embedded and the vector store is configured to do so), and stores them.\n",
    "# - It typically returns a list of IDs assigned by the vector store to each added document.\n",
    "#   These IDs can be useful for managing the documents later (e.g., updating or deleting).\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "\n",
    "# InMemoryVectorStore (in this notebook) generates new document IDs each time the program runs.\n",
    "# These IDs are not persistent across sessions, as the store only keeps data in memory.\n",
    "\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1d88d-c8c9-47e6-b897-55956c7c85eb",
   "metadata": {},
   "source": [
    "### B.Retrieval and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c182d57-36e9-4a4d-8c4f-789b3605a664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example Prompt Content (after formatting with placeholders) ---\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (question goes here) \n",
      "Context: (context goes here) \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# Import the 'hub' utility from Langchain.\n",
    "# Langchain Hub is a central repository for discovering, sharing, and using Langchain components,\n",
    "# particularly prompt templates, chains, and agents.\n",
    "from langchain import hub\n",
    "\n",
    "# --- Retrieve a Pre-defined Prompt Template ---\n",
    "# The `hub.pull()` function fetches a specific component from Langchain Hub.\n",
    "# - \"rlm/rag-prompt\": This is the identifier for the prompt being pulled.\n",
    "#   - \"rlm\" likely refers to the user or organization that published this prompt (e.g., \"Retrieval Language Model\" or a user named \"rlm\").\n",
    "#   - \"rag-prompt\" indicates that this is a prompt template specifically designed for\n",
    "#     Retrieval-Augmented Generation (RAG) tasks.\n",
    "#\n",
    "# Used pre-defined prompts from the hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# --- Inspect the Prompt Structure by Invoking it with Example Data ---\n",
    "# To understand how the prompt template is structured and what kind of output it produces\n",
    "# (before formatting for an LLM), we can invoke it with placeholder data.\n",
    "#\n",
    "# The `.invoke()` method of a Langchain prompt template typically takes a dictionary\n",
    "# where keys correspond to the input variables expected by the template.\n",
    "# For a RAG prompt, common input variables are:\n",
    "#   - \"context\": Placeholder for the retrieved documents/chunks.\n",
    "#   - \"question\": Placeholder for the user's query.\n",
    "#\n",
    "# The `.to_messages()` method converts the formatted prompt output (which might be a string\n",
    "# or a more complex structure) into a list of Langchain Message objects (e.g., HumanMessage,\n",
    "# AIMessage, SystemMessage).\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "# --- Verification and Output ---\n",
    "# Assert that the invocation results in a single message object.\n",
    "assert len(example_messages) == 1, \\\n",
    "       f\"Expected prompt invocation to result in 1 message, but got {len(example_messages)}\"\n",
    "\n",
    "# Print the content of the first (and expected only) message.\n",
    "# This allows us to see the actual text that would be sent to the LLM after the\n",
    "# placeholders for \"context\" and \"question\" are filled. \n",
    "print(\"--- Example Prompt Content (after formatting with placeholders) ---\")\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92806c5-7819-4dc6-82ff-2277c301bcd9",
   "metadata": {},
   "source": [
    "In addition to pre-defined prompt, we can use other templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1d3ae68-bc68-4d77-9f48-070c59b71c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert assistant. Answer the following question based only on the provided context.\n",
      "Do not use any outside knowledge. Be concise and accurate.\n",
      "\n",
      "Context:\n",
      "(context goes here)\n",
      "\n",
      "Question:\n",
      "(question goes here)\n",
      "\n",
      "Response:\n",
      "\n",
      "\n",
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Use three sentences maximum and keep the answer as concise as possible.\n",
      "Always say \"thanks for asking!\" at the end of the answer.\n",
      "\n",
      "(context goes here)\n",
      "\n",
      "Question: (question goes here)\n",
      "\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define a custom prompt template for question answering based on provided context\n",
    "custom_prompt = PromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "You are an expert assistant. Answer the following question based only on the provided context.\n",
    "Do not use any outside knowledge. Be concise and accurate.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Response:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Format the prompt using the provided inputs\n",
    "formatted_prompt = custom_prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "# Print the model's response\n",
    "print(formatted_prompt[0].content)\n",
    "\n",
    "#another templates\n",
    "\n",
    "print(\"\\n\")\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "formatted_prompt = custom_rag_prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "# Print the model's response\n",
    "print(formatted_prompt[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83fd6ff4-854f-47ce-a405-b079ac5ded6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document  # The standard Langchain class for representing pieces of text and their metadata.\n",
    "from typing_extensions import List, TypedDict   # `List` is for type hinting lists.\n",
    "                                              # `TypedDict` allows for defining dictionary-like structures with type hints for keys and values.\n",
    "\n",
    "# --- Define a Typed Dictionary for Representing RAG State ---\n",
    "# The `State` class is defined using `TypedDict`. This means instances of `State`\n",
    "# will be dictionaries, but with this definition, type checkers (like MyPy) and\n",
    "# IDEs can understand the expected keys and the types of their corresponding values.\n",
    "# This improves code readability, maintainability, and helps catch errors early.\n",
    "#\n",
    "# In the context of a RAG pipeline, especially one orchestrated by a graph-based\n",
    "# framework (e.g., Langchain's LangGraph), this `State` object would typically\n",
    "# carry information as it flows through different processing nodes (steps) of the graph.\n",
    "class State(TypedDict):\n",
    "    \"\"\"\n",
    "    A TypedDict representing the state of the RAG (Retrieval-Augmented Generation)\n",
    "    pipeline at various stages. This structure helps manage and pass data\n",
    "    between different components of the RAG process in a clear and type-safe manner.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    # The original question posed by the user. This is typically the starting point\n",
    "    # of the RAG pipeline and remains constant throughout the process.\n",
    "\n",
    "    context: List[Document]\n",
    "    # A list of Langchain `Document` objects. These are the chunks of text retrieved\n",
    "    # from the vector store that are deemed relevant to the `question`.\n",
    "    # This field is populated by the retrieval step. It might be initially empty or None\n",
    "    # and then updated as the pipeline progresses.\n",
    "\n",
    "    #     #eg : context :[\n",
    "    #     Document(\n",
    "    #         page_content=\"Task decomposition is the process of breaking a complex task into smaller, manageable subtasks...\",\n",
    "    #         metadata={\"source\": \"https://lilianweng.github.io/posts/2023-06-23-agent/\", \"start_index\": 0}\n",
    "    #     ),\n",
    "    #     Document(\n",
    "    #         page_content=\"One common method for task decomposition is Chain-of-Thought prompting, which guides the model to reason step by step...\",\n",
    "    #         metadata={\"source\": \"https://lilianweng.github.io/posts/2023-06-23-agent/\", \"start_index\": 1}\n",
    "    #     ),\n",
    "    #     ...\n",
    "    # ]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    answer: str\n",
    "    # The final answer generated by the Language Model (LLM) based on the\n",
    "    # `question` and the retrieved `context`. This field is populated by the\n",
    "    # generation step of the RAG pipeline. It might be initially empty or None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbc44b00-4dcf-4373-8986-9076de323adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    \"\"\"\n",
    "    Performs the retrieval step of the RAG pipeline.\n",
    "\n",
    "    This function takes the current state (which includes the user's question)\n",
    "    and uses the vector store to find relevant documents.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state of the RAG pipeline, expected to contain\n",
    "                       at least the 'question' key with the user's query.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with a 'context' key. The value is a list of\n",
    "              Langchain `Document` objects retrieved from the vector store\n",
    "              that are semantically similar to the input question.\n",
    "              This dictionary is intended to update the 'context' field in the\n",
    "              main RAG state.\n",
    "    \"\"\"\n",
    "    print(\"--- Executing Retrieval Step ---\")\n",
    "    question = state[\"question\"]\n",
    "    print(f\"Retrieving documents for question: '{question}'\")\n",
    "\n",
    "    # Perform a similarity search in the vector store using the question from the state.\n",
    "    # The `similarity_search` method returns a list of document chunks that are most relevant to the query.\n",
    "    #\n",
    "    # Although we previously stored the chunks in  'document_ids', \n",
    "    # we do not need to use those IDs here — the vector store automatically compares the embeddings of the query\n",
    "    # against all stored document vectors to find the most similar ones.\n",
    "    retrieved_docs = vector_store.similarity_search(question) \n",
    "    \n",
    "    print(f\"Retrieved {len(retrieved_docs)} documents.\")\n",
    "    if retrieved_docs:\n",
    "        print(f\"Content of the first retrieved doc (first 100 chars): '{retrieved_docs[0].page_content[:100]}...'\")\n",
    "\n",
    "    # Return a dictionary structured to update the 'context' field of the State.\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    \"\"\"\n",
    "    Performs the generation step of the RAG pipeline.\n",
    "\n",
    "    This function takes the current state (which includes the user's question\n",
    "    and the retrieved context) and uses a Language Model (LLM) to generate\n",
    "    an answer.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state of the RAG pipeline, expected to contain\n",
    "                       the 'question' and the 'context' (list of retrieved documents).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with an 'answer' key. The value is the string content\n",
    "              of the LLM's generated response. This dictionary is intended to\n",
    "              update the 'answer' field in the main RAG state.\n",
    "    \"\"\"\n",
    "    print(\"--- Executing Generation Step ---\")\n",
    "    question = state[\"question\"]\n",
    "    context_documents = state[\"context\"]\n",
    "\n",
    "    if not context_documents:\n",
    "        print(\"Warning: No context provided for generation. LLM will answer based on its general knowledge.\")\n",
    "        # Optionally, handle this case differently, e.g., return a specific message\n",
    "        # or skip LLM call if context is strictly required.\n",
    "\n",
    "    # Combine the page content of all retrieved documents into a single string.\n",
    "    # This combined text will serve as the context for the LLM.\n",
    "    # Documents are typically joined by a double newline for readability by the LLM.\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in context_documents)\n",
    "    \n",
    "    # Format the prompt with the user's question and the consolidated context.\n",
    "    # The 'prompt.invoke()' method typically prepares the input in the format\n",
    "    # expected by the LLM (e.g., a list of Message objects for chat models).\n",
    "    messages = prompt.invoke({\"question\": question, \"context\": docs_content})\n",
    "    # If `prompt` directly returns messages, this line is fine.\n",
    "    # If `prompt.invoke` returns a string and `llm` expects messages, conversion might be needed:\n",
    "    # e.g., from langchain_core.messages import HumanMessage\n",
    "    # messages = [HumanMessage(content=prompt.invoke(...))]\n",
    "\n",
    "\n",
    "    print(f\"Sending to LLM. Question: '{question}'\")\n",
    "    # Invoke the Language Model with the formatted messages (prompt + context + question).\n",
    "    response = llm.invoke(messages)\n",
    "    # The 'response' object from an LLM call often has a 'content' attribute\n",
    "    # holding the actual generated text.\n",
    "\n",
    "    # Return a dictionary structured to update the 'answer' field of the State.\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6cc104-d535-4940-ba20-2f5796f354cb",
   "metadata": {},
   "source": [
    "We'll use LangGraph to tie together the retrieval and generation steps into a single application. \n",
    "The benefits of LangGraph include:\n",
    "\n",
    "- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n",
    "- Automatic support for tracing via LangSmith and deployments via LangGraph Platform;\n",
    "- Support for persistence, human-in-the-loop, and other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be51dd4d-1dd8-4f9f-b0cc-84fe3bac999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c730363-c2a2-4d8f-b609-46e325fd3132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e46e978-0355-4514-a24e-07e5dd1a086a",
   "metadata": {},
   "source": [
    "But LangGraph is not required to build a RAG application. So we can implement the same application logic through invocations of the individual components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe248e38-269b-41fc-b218-8ecc6625f40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Executing Retrieval Step ---\n",
      "Retrieving documents for question: 'What is Task Decomposition?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 15:02:23,285 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 4 documents.\n",
      "Content of the first retrieved doc (first 100 chars): 'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each...'\n",
      "--- Executing Generation Step ---\n",
      "Sending to LLM. Question: 'What is Task Decomposition?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 15:02:24,563 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [Document(id='079be118-ae11-42c7-ad91-b9a6c0586bb9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='75250e00-d9e3-46bb-9a14-68ad6989d4c9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='514cf118-389f-4471-bba4-55a4be1cb249', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='a8b473f2-9449-468d-b691-f5649bef5c78', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.')]\n",
      "\n",
      "\n",
      "Answer: Task Decomposition involves breaking down a complex task into smaller and simpler steps, allowing for better planning and understanding. It can be achieved by prompting a language model with specific instructions or using human inputs to guide the process. Task Decomposition helps in transforming big tasks into more manageable ones, facilitating a clearer interpretation of the thinking process.\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "\n",
    "print(f'Context: {result[\"context\"]}\\n\\n')\n",
    "print(f'Answer: {result[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "480b2523-a8c0-4797-a9c6-7f8abf6beea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What is Task Decomposition?\"\n",
    "\n",
    "# retrieved_docs = vector_store.similarity_search(question)\n",
    "# docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "# prompt = prompt.invoke({\"question\": question, \"context\": docs_content})\n",
    "# answer = llm.invoke(prompt)\n",
    "#print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "227079ba-f66b-4041-a9fd-4c3508d1c5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Executing Retrieval Step ---\n",
      "Retrieving documents for question: 'What is Task Decomposition?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 15:02:43,869 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 4 documents.\n",
      "Content of the first retrieved doc (first 100 chars): 'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each...'\n",
      "{'retrieve': {'context': [Document(id='079be118-ae11-42c7-ad91-b9a6c0586bb9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='75250e00-d9e3-46bb-9a14-68ad6989d4c9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='514cf118-389f-4471-bba4-55a4be1cb249', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='a8b473f2-9449-468d-b691-f5649bef5c78', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.')]}}\n",
      "\n",
      "----------------\n",
      "\n",
      "--- Executing Generation Step ---\n",
      "Sending to LLM. Question: 'What is Task Decomposition?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 15:02:45,149 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate': {'answer': 'Task Decomposition is a technique that breaks down complex tasks into smaller and simpler steps to make them more manageable. It can be done using prompting techniques, task-specific instructions, or human inputs to guide the decomposition process. Different search processes like BFS or DFS can be used to explore multiple reasoning possibilities at each step during task decomposition.'}}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stream the steps of the StateGraph in real-time while processing the input question.\n",
    "# This allows us to track the progress of the application as it moves through the retrieve and generate steps.\n",
    "\n",
    "# Loop through the steps of the graph, streaming the updates\n",
    "for step in graph.stream(\n",
    "    {\"question\": \"What is Task Decomposition?\"},  # Input: The user's question that will be processed\n",
    "    stream_mode=\"updates\"  \n",
    "):\n",
    "\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2c8f526-b163-4c79-b41f-de5397a57c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Executing Retrieval Step ---\n",
      "Retrieving documents for question: 'What is Task Decomposition?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 15:02:45,574 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 4 documents.\n",
      "Content of the first retrieved doc (first 100 chars): 'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each...'\n",
      "--- Executing Generation Step ---\n",
      "Sending to LLM. Question: 'What is Task Decomposition?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 15:02:46,165 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Task| decomposition| is| the| process| of| breaking| down| a| problem| into| multiple| thought| steps|,| generating| multiple| thoughts| per| step|,| creating| a| tree| structure| for| exploring| reasoning| possibilities|.| It| can| be| achieved| through| L|LM| prompting|,| task|-specific| instructions|,| or| human| inputs|.| Task| decomposition| helps| transform| big| tasks| into| smaller|,| more| manageable| tasks| for| better| understanding| and| planning|.||"
     ]
    }
   ],
   "source": [
    "# Stream tokens:\n",
    "for message, metadata in graph.stream(\n",
    "    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"messages\"\n",
    "):\n",
    "    print(message.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba81cb4b-72cb-46d8-a33f-8ff8f3ad8cc2",
   "metadata": {},
   "source": [
    "## 3.Query Analysis\n",
    "\n",
    "Query analysis is the process of understanding and interpreting a user's query to determine its intent and context. It involves detecting the user's goals, recognizing key entities, and considering context to provide more accurate and relevant responses. This step is crucial in improving the efficiency of search engines, chatbots, and other information retrieval systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66063f0d-45fb-41e1-9132-bafa58387c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata of the first chunk after adding 'section': {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 8, 'section': 'beginning'}\n"
     ]
    }
   ],
   "source": [
    "# total number of document chunks\n",
    "total_documents = len(all_splits)\n",
    "\n",
    "# --- Determine approximate boundary for thirds ---\n",
    "# Integer division to find the index marking the end of the first third.\n",
    "third = total_documents // 3\n",
    "\n",
    "# --- Assign a 'section' metadata field based on chunk position ---\n",
    "for i, document in enumerate(all_splits):\n",
    "    # Assign 'beginning', 'middle', or 'end' to the 'section' metadata\n",
    "    # based on whether the chunk falls into the first, second, or final third\n",
    "    # of the document chunk list.\n",
    "    if i < third:\n",
    "        document.metadata[\"section\"] = \"beginning\"\n",
    "    elif i < 2 * third: # Check if in the second third\n",
    "        document.metadata[\"section\"] = \"middle\"\n",
    "    else: # Remaining chunks are in the final third\n",
    "        document.metadata[\"section\"] = \"end\"\n",
    "\n",
    "# --- Display metadata of the first chunk (for verification) ---\n",
    "print(f\"Metadata of the first chunk after adding 'section': {all_splits[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf35734e-1b9d-4eee-808a-31e006c5e692",
   "metadata": {},
   "source": [
    "Update the documents in our vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbd1f8c5-7cb6-458d-984c-977a8b9c7298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 15:16:10,909 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "_ = vector_store.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cb980cc-1283-4e44-ba10-5725bc381a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "class Search(TypedDict):\n",
    "    \"\"\"Search query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[\n",
    "        Literal[\"beginning\", \"middle\", \"end\"],\n",
    "        ...,\n",
    "        \"Section to query.\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3428a01-5d4a-4c94-bf18-02b942907ef2",
   "metadata": {},
   "source": [
    "Finally, we add a step to our LangGraph application to generate a query from the user's raw input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a70052a-e914-4b90-8383-2f82d9992abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def analyze_query(state: State): \n",
    "    \"\"\"\n",
    "    Analyzes the user's natural language question and converts it into a\n",
    "    structured representation using an LLM.\n",
    "\n",
    "    This step is often used in more advanced RAG pipelines to:\n",
    "    - Extract key entities or keywords for more targeted retrieval.\n",
    "    - Identify user intent or specific constraints from the query.\n",
    "    - Rephrase the query for better retrieval performance.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current RAG state, expected to contain the\n",
    "                       user's original 'question'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the 'query' key, where the value is\n",
    "              the structured output (an instance of the 'Search' Pydantic model)\n",
    "              derived from the original question. This dictionary is intended to\n",
    "              potentially update a 'query' field or a similar field in the main RAG state.\n",
    "    \"\"\"\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    query = structured_llm.invoke(state[\"question\"])\n",
    "    return {\"query\": query}\n",
    "\n",
    "\n",
    "def retrieve(state: State):\n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query[\"query\"],\n",
    "        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],\n",
    "    )\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
    "graph_builder.add_edge(START, \"analyze_query\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6651d1be-a4b6-43b3-8c6a-3347ea792dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png())) #updated imaged after adding Query Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d107e-4bda-430e-b280-205973b719b0",
   "metadata": {},
   "source": [
    "FINAL CODE :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00ffb1a4-0689-44d3-a599-2217a21574c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 15:27:53,551 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import Annotated, List, TypedDict\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "# Update metadata (illustration purposes)\n",
    "total_documents = len(all_splits)\n",
    "third = total_documents // 3\n",
    "\n",
    "for i, document in enumerate(all_splits):\n",
    "    if i < third:\n",
    "        document.metadata[\"section\"] = \"beginning\"\n",
    "    elif i < 2 * third:\n",
    "        document.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        document.metadata[\"section\"] = \"end\"\n",
    "\n",
    "\n",
    "# Index chunks\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "_ = vector_store.add_documents(all_splits)\n",
    "\n",
    "\n",
    "# Define schema for search\n",
    "class Search(TypedDict):\n",
    "    \"\"\"Search query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[\n",
    "        Literal[\"beginning\", \"middle\", \"end\"],\n",
    "        ...,\n",
    "        \"Section to query.\",\n",
    "    ]\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def analyze_query(state: State):\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    query = structured_llm.invoke(state[\"question\"])\n",
    "    return {\"query\": query}\n",
    "\n",
    "\n",
    "def retrieve(state: State):\n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query[\"query\"],\n",
    "        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],\n",
    "    )\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
    "graph_builder.add_edge(START, \"analyze_query\")\n",
    "graph = graph_builder.compile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
